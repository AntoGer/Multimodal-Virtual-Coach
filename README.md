# Multimodal-interaction
Multimodal-interaction exam project

This project aimed to develop a virtual personal trainer that thanks to two cameras can track different kinds of exercise and support the user through guided workouts and audio and video hints and corrections.

Executing the main.py script, you will launch the app and display the menu.

![Alt text](./images/selezione_esercizio.png)

Here you can select one particular exercise or start a workout:

Squat: in the quat exercise you will be asked, thanks to the voice synthesiser, to tell some information about the exercise, such as how many repetitions, sets and the recovery time all through speech interaction.
This operation has to be improved because it depends a lot on your internet connection (Google Speech recognition just records the audio and sends it to a server to analyze it) but once it is done the exercise will start.

![Alt text](./images/squat_inizio_ok.png)
